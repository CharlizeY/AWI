{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard scientific Python imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns # for visualisation\n",
    "import numpy as np\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "import random\n",
    "random.seed(15)\n",
    "\n",
    "# Default plotting parameters\n",
    "font = {'size'   : 18}\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset\n",
    "base_dir = \"/Users/Cherry0904/desktop/ArtWorldInsights/ML_modelling/\" \n",
    "Xy = pd.read_csv(base_dir + 'all_data.csv', squeeze = True)\n",
    "\n",
    "y = Xy[['logprice']]\n",
    "X_cts = Xy[['database', 'medium', 'dimensions', 'Followers Per Post (FPP)', 'Instagram performance', 'ArtfactsPresence', 'InsPresence', 'WebsitePresence']]\n",
    "X = Xy[['database', 'medium', 'dimensions', 'Followers Per Post (FPP)', 'Instagram performance', 'ArtfactsPresence', 'InsPresence', 'WebsitePresence']]\n",
    "# print(Xy.columns)\n",
    "\n",
    "# Create instance of one-hot-encoder\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Perform one-hot encoding on the columns of categorical variables \n",
    "X_encoder_df1 = pd.DataFrame(encoder.fit_transform(X[['database']]).toarray())\n",
    "X_encoder_df2 = pd.DataFrame(encoder.fit_transform(X[['medium']]).toarray())\n",
    "# print(X_encoder_df1.columns)\n",
    "X_encoder_df1.columns = ['artprice', 'artsper', 'degreeart', 'riseart', 'singulart']\n",
    "X_encoder_df2.columns = ['drawing', 'painting', 'photo']\n",
    "# print(X_encoder_df2)\n",
    "\n",
    "# Merge one-hot encoded columns back with original DataFrame\n",
    "X_final = X.join(X_encoder_df1)\n",
    "X_final = X_final.join(X_encoder_df2)\n",
    "X_final = X_final.drop(['database', 'medium'], axis=1)\n",
    "# X_final.drop(['medium'], axis=1)\n",
    "# print(X_final)\n",
    "\n",
    "# Train-test split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_final, y, test_size = 0.20 , random_state=15)\n",
    "\n",
    "# Create version with them together\n",
    "Xy_tr = pd.concat([X_tr, y_tr], axis = 1)\n",
    "\n",
    "# Normalise according to training data\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_tr)\n",
    "X_tr_sc = scaler.transform(X_tr)\n",
    "X_te_sc = scaler.transform(X_te)\n",
    "\n",
    "# Futher split to Train-validation sets - 0.8, 0.1, 0.2\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_tr, y_tr, test_size = 0.10 , random_state=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging (Bootstrap Aggregation) of Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative_mean_squared_error: -0.260\n",
      "Config: {'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "# Set up the bagging ensemble regressor \n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "regressor = BaggingRegressor()\n",
    "\n",
    "# Grid search on the number of decision trees and the maximum tree depth\n",
    "grid = {'n_estimators': [10, 25, 50, 100, 150, 200]} \n",
    "\n",
    "# Define search - maximise the defined regression metric\n",
    "search = GridSearchCV(regressor, grid, scoring='neg_mean_squared_error', cv=6, n_jobs=-1)\n",
    "\n",
    "# Perform the search\n",
    "results = search.fit(X_tr_sc, y_tr)\n",
    "\n",
    "# Summarize\n",
    "print('Negative_mean_squared_error: %.3f' % results.best_score_)      \n",
    "print('Config: %s' % results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:880: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative_mean_squared_error: -0.260\n",
      "Config: {'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "# Set up the random forest (RF) regressor \n",
    "# Compared to bagging, RF further decorrelates the decision trees by sampling subsets of features, thus reduce overfitting\n",
    "# Since the number of features for this dataset is not large, bagging and RF give similar results\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regressor = RandomForestRegressor(random_state = 0)\n",
    "\n",
    "# Grid search on the number of decision trees \n",
    "grid = {'n_estimators': [10, 25, 50, 100, 150, 200]} #, 'max_depth': [2, 3, 4]} \n",
    "\n",
    "# Define search - maximise the defined regression metric\n",
    "search = GridSearchCV(regressor, grid, scoring='neg_mean_squared_error', cv=6, n_jobs=-1)\n",
    "\n",
    "# Perform the search\n",
    "results = search.fit(X_tr_sc, y_tr)\n",
    "\n",
    "# Summarize\n",
    "print('Negative_mean_squared_error: %.3f' % results.best_score_)      \n",
    "print('Config: %s' % results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative_mean_squared_error: -0.766\n",
      "Config: {'learning_rate': 0.1, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "# boosting combines multiple weak learners into a strong learner\n",
    "\n",
    "# AdaBoost - each time a new weak learner is built, each datapoint receives a new weight depending on how well it is classified\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    " \n",
    "# Choosing Decision Tree with 1 level as the weak learner\n",
    "DTR = DecisionTreeRegressor(max_depth = 1)\n",
    "regressor = AdaBoostRegressor(base_estimator = DTR)\n",
    "\n",
    "# The learning rate is the weight applied to each regressor at each boosting iteration\n",
    "# A higher learning rate increases the contribution of each regressor\n",
    "# There is a trade-off between the learning_rate and n_estimators parameters\n",
    "grid = {'n_estimators': [10, 25, 50, 100, 150, 200], 'learning_rate':[0.01, 0.05, 0.1, 0.2]} \n",
    "\n",
    "# Define search - maximise the defined regression metric\n",
    "search = GridSearchCV(regressor, grid, scoring='neg_mean_squared_error', cv=6, n_jobs=-1)\n",
    "\n",
    "# Perform the search\n",
    "results = search.fit(X_tr_sc, y_tr)\n",
    "\n",
    "# Summarize\n",
    "print('Negative_mean_squared_error: %.3f' % results.best_score_)      \n",
    "print('Config: %s' % results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.0755           42.89s\n",
      "         2           0.9504           43.13s\n",
      "         3           0.8488           55.35s\n",
      "         4           0.7650           51.72s\n",
      "         5           0.6957           48.95s\n",
      "         6           0.6394           46.86s\n",
      "         7           0.5929           45.53s\n",
      "         8           0.5536           44.33s\n",
      "         9           0.5218           43.63s\n",
      "        10           0.4928           42.78s\n",
      "        20           0.3604           38.71s\n",
      "        30           0.3123           37.70s\n",
      "        40           0.2841           36.22s\n",
      "        50           0.2678           35.02s\n",
      "        60           0.2553           34.21s\n",
      "        70           0.2439           33.54s\n",
      "        80           0.2364           32.37s\n",
      "        90           0.2308           30.79s\n",
      "       100           0.2229           29.06s\n",
      "       200           0.1738           14.03s\n",
      "       300           0.1469            0.00s\n",
      "Negative_mean_squared_error: -0.237\n",
      "Config: {'max_depth': 10, 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "# gradient boosting - each new tree has parameters which are chosen to minimize the loss\n",
    "# This optimization step is done with gradient descent\n",
    "# That is, algorithms that optimize a cost function over function space by iteratively choosing a function (weak hypothesis) that points in the negative gradient direction\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "regressor = GradientBoostingRegressor(verbose = 1)\n",
    "\n",
    "grid = {'n_estimators': [100, 150, 200, 250, 300], 'max_depth': [2, 4, 6, 8, 10]} \n",
    "\n",
    "# Define search - maximise the defined regression metric\n",
    "search = GridSearchCV(regressor, grid, scoring='neg_mean_squared_error', cv=6, n_jobs=-1)\n",
    "\n",
    "# Perform the search\n",
    "results = search.fit(X_tr_sc, y_tr)\n",
    "\n",
    "# Summarize\n",
    "print('Negative_mean_squared_error: %.3f' % results.best_score_)      \n",
    "print('Config: %s' % results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 100 candidates, totalling 600 fits\n",
      "Negative_mean_squared_error: 0.740\n",
      "Config: {'gamma': 0.22863258080686427, 'learning_rate': 0.2826069225035944, 'max_depth': 5, 'n_estimators': 193}\n"
     ]
    }
   ],
   "source": [
    "# XGBoost - Extreme Gradient Boosting\n",
    "# XGBoost includes regression penalties in the boosting equation (like elastic net)\n",
    "# It also leverages the structure of your hardware to speed up computing times (parallelization) and facilitate memory usage\n",
    "import xgboost as xgb\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "# Gamma is the minimum loss reduction required to make a further partition on a leaf node of the tree\n",
    "# The larger gamma is, the more conservative the algorithm will be.\n",
    "params = {\n",
    "    \"gamma\": uniform(0, 0.5),\n",
    "    \"learning_rate\": uniform(0.03, 0.3), # default 0.1 \n",
    "    \"max_depth\": randint(2, 6), # default 3\n",
    "    \"n_estimators\": randint(100, 200), # default 100\n",
    "} \n",
    "# \"subsample\": uniform(0.6, 0.4)\n",
    "# \"colsample_bytree\": uniform(0.7, 0.3),\n",
    "\n",
    "# Define random search - maximise the defined regression metric\n",
    "search = RandomizedSearchCV(xgb_model, param_distributions=params, random_state=42, n_iter=100, cv=6, verbose=1, n_jobs=1, return_train_score=True)\n",
    "\n",
    "# Perform the search\n",
    "results = search.fit(X_tr_sc, y_tr)\n",
    "\n",
    "# Summarize\n",
    "print('Negative_mean_squared_error: %.3f' % results.best_score_)      \n",
    "print('Config: %s' % results.best_params_)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
